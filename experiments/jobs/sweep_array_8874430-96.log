/h/angelo/single-cell-proj/repo/team_single_cell/experiments/jobs
gpu135.cluster.local
Sun Dec 18 14:05:05 EST 2022
wandb: Starting wandb agent üïµÔ∏è
2022-12-18 14:05:14,170 - wandb.wandb_agent - INFO - Running runs: []
2022-12-18 14:05:14,782 - wandb.wandb_agent - INFO - Agent received command: run
2022-12-18 14:05:14,782 - wandb.wandb_agent - INFO - Agent starting run with config:
	atac_dim: 2500
	atac_weight: 74.41988193921405
	epochs: 60
	gex_dim: 5000
	gex_weight: 52.01606976218255
	init: he
	latent_dim: 20
	lr: 1e-05
	model: [[1000, 800, 600, 400, 200], [10, 30, 50, 100, 160], [1200, 900, 600, 400, 150], [15, 40, 60, 100, 180]]
	weight_decay: 0.05910460725603757
2022-12-18 14:05:14,789 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python gex_atac_sweep.py --atac_dim=2500 --atac_weight=74.41988193921405 --epochs=60 --gex_dim=5000 --gex_weight=52.01606976218255 --init=he --latent_dim=20 --lr=1e-05 "--model=[[1000, 800, 600, 400, 200], [10, 30, 50, 100, 160], [1200, 900, 600, 400, 150], [15, 40, 60, 100, 180]]" --weight_decay=0.05910460725603757
wandb: Currently logged in as: ange1o5 (team-single-cell). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
2022-12-18 14:05:19,803 - wandb.wandb_agent - INFO - Running runs: ['pyjvcg46']
wandb: Tracking run with wandb version 0.13.7
wandb: Run data is saved locally in /ssd003/home/angelo/single-cell-proj/repo/team_single_cell/experiments/scripts/wandb/run-20221218_140518-pyjvcg46
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-93
wandb: ‚≠êÔ∏è View project at https://wandb.ai/team-single-cell/gex_atac_sweep
wandb: üßπ View sweep at https://wandb.ai/team-single-cell/gex_atac_sweep/sweeps/j1snhit4
wandb: üöÄ View run at https://wandb.ai/team-single-cell/gex_atac_sweep/runs/pyjvcg46
 UserWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:64: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
Trying to set attribute `._uns` of view, copying.
Trying to set attribute `.var` of view, copying.
2022-12-18 14:20:59.333925: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-18 14:20:59.692816: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 FutureWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/anndata/_core/anndata.py:1228: The `inplace` parameter in pandas.Categorical.reorder_categories is deprecated and will be removed in a future version. Reordering categories will always return a new Categorical object.
... storing 'feature_types' as categorical
 FutureWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/anndata/_core/anndata.py:1228: The `inplace` parameter in pandas.Categorical.reorder_categories is deprecated and will be removed in a future version. Reordering categories will always return a new Categorical object.
... storing 'gene_id' as categorical
Reading dataset...
Feature selecting GEX...
Feature selecting ATAC...

New GEX dim: 5000;
New ATAC dim: 2501;
AnnData dataset's shape: (69249, 7501)

Initializing dataset and dataloader...
The order of labels: ['s1d1', 's1d1', 's1d1', 's1d1', 's1d1', ..., 's4d9', 's4d9', 's4d9', 's4d9', 's4d9']
Length: 69249
Categories (13, object): ['s1d1', 's1d2', 's1d3', 's2d1', ..., 's3d10', 's4d1', 's4d8', 's4d9']
The device: cuda;

The model: DeepGexAtacMultiModalAutoencoder(
  (gex_encoder): Sequential(
    (0): Linear(in_features=5000, out_features=1000, bias=True)
    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=1000, out_features=800, bias=True)
    (5): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=800, out_features=600, bias=True)
    (8): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU()
    (10): Dropout(p=0.1, inplace=False)
    (11): Linear(in_features=600, out_features=400, bias=True)
    (12): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU()
    (14): Linear(in_features=400, out_features=200, bias=True)
    (15): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU()
    (17): Dropout(p=0.1, inplace=False)
    (18): Linear(in_features=200, out_features=10, bias=True)
    (19): ReLU()
  )
  (atac_encoder): Sequential(
    (0): Linear(in_features=2501, out_features=1200, bias=True)
    (1): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=1200, out_features=900, bias=True)
    (5): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=900, out_features=600, bias=True)
    (8): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU()
    (10): Dropout(p=0.1, inplace=False)
    (11): Linear(in_features=600, out_features=400, bias=True)
    (12): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU()
    (14): Linear(in_features=400, out_features=150, bias=True)
    (15): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU()
    (17): Dropout(p=0.1, inplace=False)
    (18): Linear(in_features=150, out_features=10, bias=True)
    (19): ReLU()
  )
  (gex_decoder): Sequential(
    (0): Linear(in_features=20, out_features=10, bias=True)
    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=10, out_features=30, bias=True)
    (5): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=30, out_features=50, bias=True)
    (8): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU()
    (10): Dropout(p=0.1, inplace=False)
    (11): Linear(in_features=50, out_features=100, bias=True)
    (12): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU()
    (14): Linear(in_features=100, out_features=160, bias=True)
    (15): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU()
    (17): Dropout(p=0.1, inplace=False)
    (18): Linear(in_features=160, out_features=5000, bias=True)
    (19): ReLU()
  )
  (atac_decoder): Sequential(
    (0): Linear(in_features=20, out_features=15, bias=True)
    (1): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=15, out_features=40, bias=True)
    (5): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=40, out_features=60, bias=True)
    (8): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU()
    (10): Dropout(p=0.1, inplace=False)
    (11): Linear(in_features=60, out_features=100, bias=True)
    (12): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU()
    (14): Linear(in_features=100, out_features=180, bias=True)
    (15): BatchNorm1d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU()
    (17): Dropout(p=0.1, inplace=False)
    (18): Linear(in_features=180, out_features=2501, bias=True)
    (19): Sigmoid()
  )
);
The dataset: <datasets.anndatadataset.AnnDataDataset object at 0x7fde42b129d0>;
The optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0.05910460725603757
);

Epoch 0 recon loss: 4466.6336449863
Epoch 1 recon loss: 4395.5738939381
Epoch 2 recon loss: 4220.1268023151
Epoch 3 recon loss: 4401.3188930359
Epoch 4 recon loss: 4386.1216384571
Epoch 5 recon loss: 4311.5935345465
Epoch 6 recon loss: 4220.2731689405
Epoch 7 recon loss: 4374.5086364924
Epoch 8 recon loss: 4489.9135670755
Epoch 9 recon loss: 4227.9550729809
Epoch 10 recon loss: 4331.895248924
Epoch 11 recon loss: 4238.6779258178
Epoch 12 recon loss: 4267.2452959192
Epoch 13 recon loss: 4296.7386525627
Epoch 14 recon loss: 4452.7398601454
Epoch 15 recon loss: 4266.0388372056
Epoch 16 recon loss: 4327.9540117259
Epoch 17 recon loss: 4123.8441516835
Epoch 18 recon loss: 4469.3612042807
Epoch 19 recon loss: 4290.9855011345
Epoch 20 recon loss: 4346.9267759387
Epoch 21 recon loss: 4436.8621890971
Epoch 22 recon loss: 4359.1223335814
Epoch 23 recon loss: 4213.9167740865
Epoch 24 recon loss: 4083.2088997272
Epoch 25 recon loss: 4425.5179129877
Epoch 26 recon loss: 4400.917048758
Epoch 27 recon loss: 4358.0934539152
Epoch 28 recon loss: 4344.110023292
Epoch 29 recon loss: 4288.6848689685
Epoch 30 recon loss: 4150.8119280272
Epoch 31 recon loss: 4284.983436321
Epoch 32 recon loss: 4381.1545367325
Epoch 33 recon loss: 4399.7082920326
Epoch 34 recon loss: 4287.196092241
Epoch 35 recon loss: 4290.9810325547
Epoch 36 recon loss: 4402.916268555
Epoch 37 recon loss: 4374.4231656651
Epoch 38 recon loss: 4278.9744538842
Epoch 39 recon loss: 4341.8853866356
Epoch 40 recon loss: 4429.5094420016
Epoch 41 recon loss: 4240.115443014
Epoch 42 recon loss: 4405.1315584566
Epoch 43 recon loss: 4236.1851168631
Epoch 44 recon loss: 4301.1817937566
Epoch 45 recon loss: 4274.9952963029
Epoch 46 recon loss: 4208.5776667452
Epoch 47 recon loss: 4332.2004242561
Epoch 48 recon loss: 4328.2430066777
Epoch 49 recon loss: 4169.370084757
Epoch 50 recon loss: 4196.6274737757
Epoch 51 recon loss: 4180.3063666138
Epoch 52 recon loss: 4338.5823834871
Epoch 53 recon loss: 4241.3481018496
Epoch 54 recon loss: 4346.4805256101
Epoch 55 recon loss: 4296.5542126035
Epoch 56 recon loss: 4322.3090479185
Epoch 57 recon loss: 4155.1814228459
Epoch 58 recon loss: 4440.1051077545
Epoch 59 recon loss: 4345.7639405136

Latent space type: <class 'numpy.ndarray'>;
GEX encoded shape: torch.Size([69249, 10]);
ATAC encoded shape: torch.Size([69249, 10]);
Latent space shape: (69249, 20);

0.43180134105879564
   celltype_ari  celltype_ami  ...  batch_homogeneity  batch_complete
0      0.027314      0.114267  ...           0.913597         0.93525

[1 rows x 8 columns]
2022-12-18 14:23:40,028 - wandb.wandb_agent - INFO - Cleaning up finished run: pyjvcg46
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: \ 0.007 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: | 0.029 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  recon loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÜ
wandb: total_score ‚ñÅ
wandb: 
wandb: Run summary:
wandb:  recon loss 4345.76394
wandb: total_score 0.4318
wandb: 
wandb: Synced comic-sweep-93: https://wandb.ai/team-single-cell/gex_atac_sweep/runs/pyjvcg46
wandb: Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221218_140518-pyjvcg46/logs
2022-12-18 14:23:45,309 - wandb.wandb_agent - INFO - Agent received command: run
2022-12-18 14:23:45,314 - wandb.wandb_agent - INFO - Agent starting run with config:
	atac_dim: 5000
	atac_weight: 74.38473692893575
	epochs: 20
	gex_dim: 5000
	gex_weight: 4.1436321288050175
	init: xavier
	latent_dim: 10
	lr: 1e-05
	model: [[1200, 400, 100], [50, 100, 200], [1600, 600, 120], [60, 100, 150]]
	weight_decay: 0.08574565420485585
2022-12-18 14:23:45,522 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python gex_atac_sweep.py --atac_dim=5000 --atac_weight=74.38473692893575 --epochs=20 --gex_dim=5000 --gex_weight=4.1436321288050175 --init=xavier --latent_dim=10 --lr=1e-05 "--model=[[1200, 400, 100], [50, 100, 200], [1600, 600, 120], [60, 100, 150]]" --weight_decay=0.08574565420485585
2022-12-18 14:23:50,618 - wandb.wandb_agent - INFO - Running runs: ['ty82vln0']
wandb: Currently logged in as: ange1o5 (team-single-cell). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.7
wandb: Run data is saved locally in /ssd003/home/angelo/single-cell-proj/repo/team_single_cell/experiments/scripts/wandb/run-20221218_142358-ty82vln0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-425
wandb: ‚≠êÔ∏è View project at https://wandb.ai/team-single-cell/gex_atac_sweep
wandb: üßπ View sweep at https://wandb.ai/team-single-cell/gex_atac_sweep/sweeps/j1snhit4
wandb: üöÄ View run at https://wandb.ai/team-single-cell/gex_atac_sweep/runs/ty82vln0
 UserWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:64: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
Trying to set attribute `._uns` of view, copying.
Trying to set attribute `.var` of view, copying.
2022-12-18 14:31:41.796411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-18 14:31:42.227910: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
slurmstepd: error: *** JOB 8874526 ON gpu135 CANCELLED AT 2022-12-18T14:32:15 ***
