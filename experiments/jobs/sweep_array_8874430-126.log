/h/angelo/single-cell-proj/repo/team_single_cell/experiments/jobs
gpu127.cluster.local
Sun Dec 18 14:05:28 EST 2022
wandb: Starting wandb agent üïµÔ∏è
2022-12-18 14:05:37,641 - wandb.wandb_agent - INFO - Running runs: []
2022-12-18 14:05:38,083 - wandb.wandb_agent - INFO - Agent received command: run
2022-12-18 14:05:38,083 - wandb.wandb_agent - INFO - Agent starting run with config:
	atac_dim: 5000
	atac_weight: 81.30027953584154
	epochs: 80
	gex_dim: 2500
	gex_weight: 37.34673972974296
	init: xavier
	latent_dim: 10
	lr: 0.001
	model: [[1000, 200], [50, 100], [2000, 400], [100, 300]]
	weight_decay: 0.035611538283574176
2022-12-18 14:05:38,090 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python gex_atac_sweep.py --atac_dim=5000 --atac_weight=81.30027953584154 --epochs=80 --gex_dim=2500 --gex_weight=37.34673972974296 --init=xavier --latent_dim=10 --lr=0.001 "--model=[[1000, 200], [50, 100], [2000, 400], [100, 300]]" --weight_decay=0.035611538283574176
2022-12-18 14:05:43,102 - wandb.wandb_agent - INFO - Running runs: ['m6iz09jz']
wandb: Currently logged in as: ange1o5 (team-single-cell). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.7
wandb: Run data is saved locally in /ssd003/home/angelo/single-cell-proj/repo/team_single_cell/experiments/scripts/wandb/run-20221218_140543-m6iz09jz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-129
wandb: ‚≠êÔ∏è View project at https://wandb.ai/team-single-cell/gex_atac_sweep
wandb: üßπ View sweep at https://wandb.ai/team-single-cell/gex_atac_sweep/sweeps/j1snhit4
wandb: üöÄ View run at https://wandb.ai/team-single-cell/gex_atac_sweep/runs/m6iz09jz
 UserWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:64: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
Trying to set attribute `._uns` of view, copying.
Trying to set attribute `.var` of view, copying.
2022-12-18 14:25:49.897715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-18 14:25:50.007603: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 FutureWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/anndata/_core/anndata.py:1228: The `inplace` parameter in pandas.Categorical.reorder_categories is deprecated and will be removed in a future version. Reordering categories will always return a new Categorical object.
... storing 'feature_types' as categorical
 FutureWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/anndata/_core/anndata.py:1228: The `inplace` parameter in pandas.Categorical.reorder_categories is deprecated and will be removed in a future version. Reordering categories will always return a new Categorical object.
... storing 'gene_id' as categorical
Reading dataset...
Feature selecting GEX...
Feature selecting ATAC...

New GEX dim: 2500;
New ATAC dim: 5001;
AnnData dataset's shape: (69249, 7501)

Initializing dataset and dataloader...
The order of labels: ['s1d1', 's1d1', 's1d1', 's1d1', 's1d1', ..., 's4d9', 's4d9', 's4d9', 's4d9', 's4d9']
Length: 69249
Categories (13, object): ['s1d1', 's1d2', 's1d3', 's2d1', ..., 's3d10', 's4d1', 's4d8', 's4d9']
The device: cuda;

The model: DeepGexAtacMultiModalAutoencoder(
  (gex_encoder): Sequential(
    (0): Linear(in_features=2500, out_features=1000, bias=True)
    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=1000, out_features=200, bias=True)
    (5): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=200, out_features=5, bias=True)
    (8): ReLU()
  )
  (atac_encoder): Sequential(
    (0): Linear(in_features=5001, out_features=2000, bias=True)
    (1): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=2000, out_features=400, bias=True)
    (5): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=400, out_features=5, bias=True)
    (8): ReLU()
  )
  (gex_decoder): Sequential(
    (0): Linear(in_features=10, out_features=50, bias=True)
    (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=50, out_features=100, bias=True)
    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=100, out_features=2500, bias=True)
    (8): ReLU()
  )
  (atac_decoder): Sequential(
    (0): Linear(in_features=10, out_features=100, bias=True)
    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=300, bias=True)
    (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=300, out_features=5001, bias=True)
    (8): Sigmoid()
  )
);
The dataset: <datasets.anndatadataset.AnnDataDataset object at 0x7f22e840d210>;
The optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.035611538283574176
);

Epoch 0 recon loss: 6263.2310351271
Epoch 1 recon loss: 6019.8439595261
Epoch 2 recon loss: 5967.358799107
Epoch 3 recon loss: 6029.6881478716
Epoch 4 recon loss: 5588.2404288601
Epoch 5 recon loss: 5659.9476528865
Epoch 6 recon loss: 5545.2174455185
Epoch 7 recon loss: 5399.9727287874
Epoch 8 recon loss: 4987.9866659825
Epoch 9 recon loss: 5165.556954416
Epoch 10 recon loss: 5076.6292724473
Epoch 11 recon loss: 4961.1928597612
Epoch 12 recon loss: 4832.9987811707
Epoch 13 recon loss: 4691.8999080303
Epoch 14 recon loss: 4388.2428703776
Epoch 15 recon loss: 4482.8392708408
Epoch 16 recon loss: 4374.9066249441
Epoch 17 recon loss: 4285.8810752186
Epoch 18 recon loss: 3907.3320103894
Epoch 19 recon loss: 3865.7867378874
Epoch 20 recon loss: 3763.6163637447
Epoch 21 recon loss: 3681.5747593851
Epoch 22 recon loss: 3447.8816126701
Epoch 23 recon loss: 3553.7161397617
Epoch 24 recon loss: 3281.8444168143
Epoch 25 recon loss: 3275.5775783806
Epoch 26 recon loss: 3278.6465009931
Epoch 27 recon loss: 3139.3772874553
Epoch 28 recon loss: 3030.8777211151
Epoch 29 recon loss: 2919.4137658597
Epoch 30 recon loss: 2886.6076206342
Epoch 31 recon loss: 2574.9987175242
Epoch 32 recon loss: 2734.2730829106
Epoch 33 recon loss: 2632.8093275598
Epoch 34 recon loss: 2466.8863650902
Epoch 35 recon loss: 2348.5464442241
Epoch 36 recon loss: 2340.71294536
Epoch 37 recon loss: 2307.0208227674
Epoch 38 recon loss: 2221.1516522118
Epoch 39 recon loss: 2212.9592756203
Epoch 40 recon loss: 2043.6133843132
Epoch 41 recon loss: 2002.2466390186
Epoch 42 recon loss: 2089.9510569021
Epoch 43 recon loss: 1942.2420320506
Epoch 44 recon loss: 1834.8315268872
Epoch 45 recon loss: 1828.2453629413
Epoch 46 recon loss: 1724.0955883486
Epoch 47 recon loss: 1686.4174987017
Epoch 48 recon loss: 1690.6055042735
Epoch 49 recon loss: 1767.212928666
Epoch 50 recon loss: 1569.3094129123
Epoch 51 recon loss: 1623.4675633963
Epoch 52 recon loss: 1467.8185071808
Epoch 53 recon loss: 1504.1788287127
Epoch 54 recon loss: 1335.1003901321
Epoch 55 recon loss: 1365.1818331861
Epoch 56 recon loss: 1469.6429238767
Epoch 57 recon loss: 1341.0961327181
Epoch 58 recon loss: 1280.9006792985
Epoch 59 recon loss: 1184.7070078195
Epoch 60 recon loss: 1340.2602598981
Epoch 61 recon loss: 1156.670244471
Epoch 62 recon loss: 1100.6282312272
Epoch 63 recon loss: 1197.0191274849
Epoch 64 recon loss: 1046.8412863506
Epoch 65 recon loss: 991.7743812537
Epoch 66 recon loss: 943.1687836195
Epoch 67 recon loss: 859.7914846798
Epoch 68 recon loss: 886.0038584647
Epoch 69 recon loss: 1002.1034776471
Epoch 70 recon loss: 732.5591267546
Epoch 71 recon loss: 823.5937474413
Epoch 72 recon loss: 806.2464086808
Epoch 73 recon loss: 732.1900257825
Epoch 74 recon loss: 795.6904703694
Epoch 75 recon loss: 693.3776555958
Epoch 76 recon loss: 679.9488818908
Epoch 77 recon loss: 702.7676780851
Epoch 78 recon loss: 687.1324210428
Epoch 79 recon loss: 651.0631801068

Latent space type: <class 'numpy.ndarray'>;
GEX encoded shape: torch.Size([69249, 5]);
ATAC encoded shape: torch.Size([69249, 5]);
Latent space shape: (69249, 10);

0.5079254623997155
   celltype_ari  celltype_ami  ...  batch_homogeneity  batch_complete
0      0.091185      0.331932  ...           0.774812        0.856065

[1 rows x 8 columns]
2022-12-18 14:28:52,978 - wandb.wandb_agent - INFO - Cleaning up finished run: m6iz09jz
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: \ 0.007 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:  recon loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: total_score ‚ñÅ
wandb: 
wandb: Run summary:
wandb:  recon loss 651.06318
wandb: total_score 0.50793
wandb: 
wandb: Synced eager-sweep-129: https://wandb.ai/team-single-cell/gex_atac_sweep/runs/m6iz09jz
wandb: Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221218_140543-m6iz09jz/logs
2022-12-18 14:28:57,898 - wandb.wandb_agent - INFO - Agent received command: run
2022-12-18 14:28:57,899 - wandb.wandb_agent - INFO - Agent starting run with config:
	atac_dim: 10000
	atac_weight: 75.0717420069899
	epochs: 40
	gex_dim: 1000
	gex_weight: 31.44955113726368
	init: he
	latent_dim: 128
	lr: 0.001
	model: [[1000, 200], [50, 100], [2000, 400], [100, 300]]
	weight_decay: 0.017620491638359593
2022-12-18 14:28:58,070 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python gex_atac_sweep.py --atac_dim=10000 --atac_weight=75.0717420069899 --epochs=40 --gex_dim=1000 --gex_weight=31.44955113726368 --init=he --latent_dim=128 --lr=0.001 "--model=[[1000, 200], [50, 100], [2000, 400], [100, 300]]" --weight_decay=0.017620491638359593
wandb: Currently logged in as: ange1o5 (team-single-cell). Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
2022-12-18 14:29:03,260 - wandb.wandb_agent - INFO - Running runs: ['vdp4mytg']
wandb: Tracking run with wandb version 0.13.7
wandb: Run data is saved locally in /ssd003/home/angelo/single-cell-proj/repo/team_single_cell/experiments/scripts/wandb/run-20221218_142902-vdp4mytg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-481
wandb: ‚≠êÔ∏è View project at https://wandb.ai/team-single-cell/gex_atac_sweep
wandb: üßπ View sweep at https://wandb.ai/team-single-cell/gex_atac_sweep/sweeps/j1snhit4
wandb: üöÄ View run at https://wandb.ai/team-single-cell/gex_atac_sweep/runs/vdp4mytg
 UserWarning:/h/angelo/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:64: `flavor='seurat_v3'` expects raw count data, but non-integers were found.
Trying to set attribute `._uns` of view, copying.
Trying to set attribute `.var` of view, copying.
slurmstepd: error: *** JOB 8874556 ON gpu127 CANCELLED AT 2022-12-18T14:32:15 ***
