{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "postal-asbestos",
   "metadata": {},
   "source": [
    "The aim of this notebook is to introduce an AutoEncoder model for multi-modal integration of the ATAC + GEX data. The multi-modal AutoEncoder idea will be taken from (https://ieeexplore.ieee.org/document/8715409) - specifically Figure 4.\n",
    "\n",
    "We'll extend this idea to the gene-expression and atac-seq data. We'll select variable features for both modalities and then jointly encode the gex and atac-seq data into the same space. After training and monitoring the loss to a certain extent, we'll freeze model training and get the embeddings (from the latent space) for the multi-modal data and then test to see how well this does based on our evaluation (and see if it's better than simply concatenating the PCA reduction of each modality individually)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-afternoon",
   "metadata": {},
   "source": [
    "Let's get started by loading the single-cell libraries, the data, and getting the variable features from both modalities. We'll restrict ourselves to 2500 features for GEX and 5000 features for ATAC/chromatin accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seven-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc \n",
    "import anndata as ann\n",
    "import episcanpy as esc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aboriginal-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "multiome = sc.read_h5ad(\"data/multimodal/GSE194122_openproblems_neurips2021_multiome_BMMC_processed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civilian-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " UserWarning:/h/hmaan/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:64: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
      "Trying to set attribute `._uns` of view, copying.\n"
     ]
    }
   ],
   "source": [
    "gex = multiome[:, multiome.var[\"feature_types\"] == \"GEX\"] # Subset all data, not just the counts \n",
    "sc.pp.highly_variable_genes(gex, n_top_genes=2500, flavor=\"seurat_v3\") # Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "arranged-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.var` of view, copying.\n"
     ]
    }
   ],
   "source": [
    "atac = multiome[:, multiome.var[\"feature_types\"] == \"ATAC\"] # Subset all data, not just the counts \n",
    "esc.pp.select_var_feature(atac, nb_features=5000, show=False) # Feature-selection - most variable features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-kitchen",
   "metadata": {},
   "source": [
    "Let's go ahead and load the tensorflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stopped-replication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 17:17:30.798600: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-necklace",
   "metadata": {},
   "source": [
    "We'll extract our GEX and ATAC data and load it into tensorflow tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "instant-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvg_indices = gex.var[\"highly_variable\"]\n",
    "gex_arr = gex.X.todense()[:, hvg_indices]\n",
    "gex_tensor = tf.convert_to_tensor(gex_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "crazy-plenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([69249, 2500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gex_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sought-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_arr = atac.X.todense()\n",
    "atac_tensor = tf.convert_to_tensor(atac_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "spiritual-roommate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([69249, 5001])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atac_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-familiar",
   "metadata": {},
   "source": [
    "One extra dimension in the ATAC data, but we don't need to worry about that. We'll just say that the dimensionality of our ATAC data is 5001. We'll input both datatypes as a concatenated representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "preliminary-adelaide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([69249, 7501])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gex_atac_concat = tf.concat([gex_tensor, atac_tensor], axis = 1)\n",
    "gex_atac_concat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-writer",
   "metadata": {},
   "source": [
    "Lets go ahead and create our AutoEncoder model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "through-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 20 # Specify the size of our latent dimension \n",
    "gex_dim = 2500\n",
    "atac_dim = 5001\n",
    "\n",
    "# Create the model class for our AutoEncoder - this follows mostly for the tutorial from \n",
    "# https://www.tensorflow.org/tutorials/generative/autoencoder, except we're putting a multi-modal\n",
    "# flavor on it and ensuring it uses and reconstructs both GEX and ATAC outputs \n",
    "class MultiModalAutoencoder(Model):\n",
    "    def __init__(self, latent_dim, gex_dim, atac_dim):\n",
    "        super(MultiModalAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gex_dim = gex_dim\n",
    "        self.atac_dim = atac_dim\n",
    "        # We have two encoders and decoder - for each modality\n",
    "        # We divide latent dim by two because we are going to \n",
    "        # concatenate the two modalities in latent space and\n",
    "        # then use that concatenated representation to reconstruct\n",
    "        # each modality \n",
    "        self.gex_encoder = tf.keras.Sequential([\n",
    "            layers.Dense(250, activation=\"relu\"),\n",
    "            layers.Dense(latent_dim/2, activation=\"relu\")\n",
    "        ])\n",
    "        self.atac_encoder = tf.keras.Sequential([\n",
    "            layers.Dense(250, activation=\"relu\"),\n",
    "            layers.Dense(latent_dim/2, activation=\"relu\")\n",
    "        ])\n",
    "        self.latent_concat = tf.keras.layers.Concatenate(\n",
    "            axis=-1\n",
    "        )\n",
    "        self.outputs_concat = tf.keras.layers.Concatenate(\n",
    "            axis=-1\n",
    "        )\n",
    "        self.gex_decoder = tf.keras.Sequential([\n",
    "            layers.Dense(250, activation = \"relu\"),\n",
    "            layers.Dense(gex_dim, activation = \"relu\")\n",
    "        ])\n",
    "        self.atac_decoder = tf.keras.Sequential([\n",
    "            layers.Dense(250, activation = \"relu\"),\n",
    "            layers.Dense(atac_dim, activation = \"relu\")\n",
    "        ])\n",
    "        \n",
    "    def call(self, gex_atac_X):\n",
    "        # Extract the data\n",
    "        gex_X = gex_atac_X[:, 0:2501]\n",
    "        atac_X = gex_atac_X[:, 2501:]\n",
    "        # Encode both the GEX and ATAC data \n",
    "        gex_Z = self.gex_encoder(gex_X)\n",
    "        atac_Z = self.atac_encoder(atac_X)\n",
    "        # Concatenate the two encoded modalities \n",
    "        gex_atac_c = self.latent_concat([gex_Z, atac_Z]) # This is our latent we'll use later\n",
    "        # Use the concatenated representation to recover both GEx and ATAC\n",
    "        gex_X_decoded = self.gex_decoder(gex_Z)\n",
    "        atac_X_decoded = self.atac_decoder(atac_Z)\n",
    "        gex_atac_X_decoded = self.outputs_concat([gex_X_decoded, atac_X_decoded])\n",
    "        return gex_atac_X_decoded \n",
    "    \n",
    "# We're going to define a custom loss as we need a separate loss for both modalities \n",
    "# For GEX, since the data is continuous, we can use a mean-squared error loss \n",
    "# For ATAC, since the data is binary, we'll use a binarycrossentropy loss \n",
    "# We'll combine these to have even weight for now - but the scaling can be played around with\n",
    "# (And maybe even treated as a hyperparameter)\n",
    "def multimodal_loss(gex_atac_true, gex_atac_pred):\n",
    "    # GEX loss \n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    gex_true = gex_atac_true[:, 0:2501]\n",
    "    gex_pred = gex_atac_pred[:, 0:2501]\n",
    "    gex_loss = mse(gex_true, gex_pred)\n",
    "    \n",
    "    # ATAC loss \n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    atac_true = gex_atac_true[:, 2501:]\n",
    "    atac_pred = gex_atac_pred[:, 2501:]\n",
    "    atac_loss = bce(atac_true, atac_pred)\n",
    "    \n",
    "    # Combine both and return\n",
    "    loss = gex_loss*0.5 + atac_loss*0.5\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-enlargement",
   "metadata": {},
   "source": [
    "Let's compile the autoencoder and train for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "floppy-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = MultiModalAutoencoder(latent_dim, gex_dim, atac_dim)\n",
    "autoencoder.compile(optimizer='adam', loss=multimodal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "drawn-dynamics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "340/542 [=================>............] - ETA: 13s - loss: 8.7810"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19317/4091241499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                )\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2497\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1863\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/single_cell_env/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder.fit(gex_atac_concat, gex_atac_concat,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                batch_size=128\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "traditional-chase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([66748, 7501])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "provincial-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.        0.        0.        ... 0.        1.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 1.        0.        0.       ]\n",
      " ...\n",
      " [0.        0.        0.        ... 0.        1.        1.       ]\n",
      " [0.        0.        0.        ... 0.        1.        1.       ]\n",
      " [0.        0.        2.0074675 ... 0.        0.        1.       ]], shape=(69249, 7501), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(gex_atac_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-worthy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "single_cell_env",
   "language": "python",
   "name": "single_cell_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
